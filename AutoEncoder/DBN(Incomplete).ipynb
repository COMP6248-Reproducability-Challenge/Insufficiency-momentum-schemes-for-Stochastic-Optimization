{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DBN(Incomplete).ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"F4V6NYPpYUfv","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"erM0-ebYN-uc","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import torch\n","\n","class RBM(nn.Module):\n","    def __init__(self,\n","                 n_visible=256,\n","                 n_hidden=64):\n","        super(RBM, self).__init__()\n","        self.W = nn.Parameter(torch.Tensor(n_hidden,n_visible).uniform_(-1.0/(n_visible+n_hidden), 1.0/(n_visible+n_hidden)))\n","        self.v_bias = nn.Parameter(torch.zeros(n_visible))\n","        self.h_bias = nn.Parameter(torch.zeros(n_hidden))\n","    \n","    def sample_from_p(self,p):\n","        return torch.bernoulli(p)\n","    \n","    def v_to_h(self,v):\n","#         print(21212121212)\n","        # p_h = F.sigmoid(v.mm(self.W.t()) + self.h_bias.repeat(v.size()[0],1))\n","#         print(self.v_bias.shape)\n","#         print(self.h_bias.shape)\n","#         print(v.shape)\n","#         print(F.linear(v,self.W,self.h_bias).shape)\n","#         print(self.W.shape)\n","        X_prob = torch.matmul(v,self.W.t())\n","        X_prob = torch.add(X_prob, self.h_bias)#W.x + c\n","        if n_hidden == 30:\n","          p_h = F.relu(X_prob, inplace=False)\n","        else:\n","          p_h = torch.sigmoid(X_prob)\n","        \n","#         print(333333333)\n","        sample_h = self.sample_from_p(p_h)\n","#         print(444444444)\n","        return p_h,sample_h\n","    \n","    def h_to_v(self,h):\n","        X_prob = F.linear(h,self.W.t(),self.v_bias)\n","        if n_hidden == 30:\n","          p_v = F.relu(X_prob, inplace=False)\n","        else:\n","          p_v = torch.sigmoid(X_prob)\n","        sample_v = self.sample_from_p(p_v)\n","        return p_v,sample_v\n","        \n","    def forward(self,v, CD_k = 10):\n","#         print(v.shape)\n","#         print(222222222)\n","        pre_h1,h1 = self.v_to_h(v)\n","#         print(pre_h1.shape)\n","#         print(h1.shape)\n","        h_ = h1\n","        for _ in range(CD_k):\n","            pre_v_,v_ = self.h_to_v(h_)\n","            pre_h_,h_ = self.v_to_h(v_)\n","        \n","        return v,v_\n","    \n","    def free_energy(self,v):\n","        vbias_term = v.mv(self.v_bias)\n","        wx_b = torch.clamp(F.linear(v,self.W,self.h_bias),-80,80)\n","        hidden_term = wx_b.exp().add(1).log().sum(1)\n","        return (-hidden_term - vbias_term).mean()\n","      \n","    def reconstruct(self , X,n_gibbs):\n","        '''\n","        This will reconstruct the sample with k steps of gibbs Sampling\n","        '''\n","        v = X\n","        for i in range(n_gibbs):\n","            prob_h_,h = self.v_to_h(v)\n","            prob_v_,v = self.h_to_v(prob_h_)\n","        return prob_v_,v\n","      \n","    def reconstruction_error(self, input_data,\n","                                n_gibbs_sampling_steps=1,lr = 0.001):\n","        # positive phase\n","\n","        positive_hidden_probabilities,positive_hidden_act  = self.h_to_v(input_data)\n","\n","        # calculating W via positive side\n","        positive_associations = torch.matmul(input_data.t() , positive_hidden_act)\n","\n","\n","        # negetive phase\n","        hidden_activations = positive_hidden_act\n","        for i in range(n_gibbs_sampling_steps):\n","            visible_probabilities , _ = self.v_to_h(hidden_activations)\n","            hidden_probabilities,hidden_activations = self.h_to_v(visible_probabilities)\n","\n","        negative_visible_probabilities = visible_probabilities\n","        negative_hidden_probabilities = hidden_probabilities\n","\n","        # calculating W via negative side\n","        negative_associations = torch.matmul(negative_visible_probabilities.t() , negative_hidden_probabilities)\n","\n","\n","        # Compute reconstruction error\n","        error = torch.mean(torch.sum((input_data - negative_visible_probabilities)**2 , dim = 0))\n","\n","        return error,torch.sum(torch.abs(grad_update))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H7VF5xP6OGvL","colab_type":"code","colab":{}},"source":["class DBN(nn.Module):\n","    def __init__(self,\n","                 n_visible=784,\n","                 n_hidden=[23*23 ,18*18],):\n","        super(DBN, self).__init__()\n","        \n","        self.n_layers = len(n_hidden)\n","        self.rbm_layers = []\n","        \n","        for i in range(self.n_layers):\n","            if i == 0:\n","                input_size = n_visible\n","            else:\n","                input_size = n_hidden[i-1]\n","            rbm = RBM(n_visible = input_size, n_hidden = n_hidden[i])\n","            \n","            self.rbm_layers.append(rbm)\n","        \n","        self.W_rec = [nn.Parameter(self.rbm_layers[i].W.data.clone()) for i in range(self.n_layers-1)]\n","        self.W_gen = [nn.Parameter(self.rbm_layers[i].W.data) for i in range(self.n_layers-1)]\n","        self.bias_rec = [nn.Parameter(self.rbm_layers[i].h_bias.data.clone()) for i in range(self.n_layers-1)]\n","        self.bias_gen = [nn.Parameter(self.rbm_layers[i].v_bias.data) for i in range(self.n_layers-1)]\n","        self.W_mem = nn.Parameter(self.rbm_layers[-1].W.data)\n","        self.v_bias_mem = nn.Parameter(self.rbm_layers[-1].v_bias.data)\n","        self.h_bias_mem = nn.Parameter(self.rbm_layers[-1].h_bias.data)\n","        \n","        for i in range(self.n_layers-1):\n","            self.register_parameter('W_rec%i'%i, self.W_rec[i])\n","            self.register_parameter('W_gen%i'%i, self.W_gen[i])\n","            self.register_parameter('bias_rec%i'%i, self.bias_rec[i])\n","            self.register_parameter('bias_gen%i'%i, self.bias_gen[i])\n","        \n","        \n","    def forward(self, v_input, ith_layer, CD_k = 10): #for greedy training\n","        v = v_input\n","        \n","        for ith in range(ith_layer):\n","            p_v, v = self.rbm_layers[ith].v_to_h(v)\n","            \n","        v, v_ = self.rbm_layers[ith_layer](v, CD_k = CD_k)\n","\n","        return v, v_\n","      \n","    def reconstruct(self,input_data):\n","      '''\n","      go till the final layer and then reconstruct\n","      '''\n","      h = input_data\n","      p_h = 0\n","      for i in range(len(self.rbm_layers)):\n","          h = h.view((h.shape[0] , -1)).type(torch.FloatTensor)#flatten\n","          p_h,h = self.rbm_layers[i].v_to_h(h)\n","\n","      v = h\n","      for i in range(len(self.rbm_layers)-1,-1,-1):\n","          v = v.view((v.shape[0] , -1)).type(torch.FloatTensor)\n","          p_v,v = self.rbm_layers[i].h_to_v(v)\n","      return p_v,v"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IVk-N5B9Y8i5","colab_type":"code","outputId":"26aed5ec-82f3-4b07-df9b-6c3ea03b6fa4","colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["import torch \n","import torchvision\n","from torchvision import datasets,transforms\n","from torch.utils.data import Dataset,DataLoader\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","import math\n","import numpy as np\n","\n","batch_size = 8\n","\n","# convert each image to tensor format\n","transform = transforms.Compose([\n","    transforms.ToTensor()  # convert to tensor\n","])\n","\n","\n","\n","mnist_data = datasets.MNIST('../data', train=True, download=True, transform=transform)\n","test_data = datasets.MNIST('../data', train=False, download=True, transform=transform)\n","\n","#Lets us visualize a number from the data set\n","idx = 5\n","img = mnist_data.train_data[idx]\n","print(\"The number shown is the number: {}\".format(mnist_data.train_labels[idx]) )\n","plt.imshow(img , cmap = 'gray')\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The number shown is the number: 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n","  warnings.warn(\"train_data has been renamed data\")\n","/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n","  warnings.warn(\"train_labels has been renamed targets\")\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"vRyqakmJM9Op","colab_type":"code","outputId":"022c05c7-a026-4c74-eb7c-0c338b29f43e","colab":{"base_uri":"https://localhost:8080/","height":4413}},"source":["import torch\n","import torch.autograd as autograd\n","import numpy as np\n","import torch.utils.data\n","from torch import optim\n","from torch.autograd import Variable\n","from joblib import Parallel, delayed\n","import multiprocessing\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","# optimizer = optim.Adam(dbn_mnist.parameters(), lr=1e-3)\n","\n","# loss_function = nn.CrossEntropyLoss()\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","\n","lr = 0.1\n","epoch = 10\n","batch_size = 8\n","input_data = mnist_data\n","weight_decay = 0\n","L1_penalty = 0\n","CD_k = 10\n","test_set = test_data\n","initialize_v = False\n","\n","n_hidden=[784,1000,300,30,300,1000,784]\n","\n","dbn = DBN(n_hidden=n_hidden)\n","train_loader = torch.utils.data.DataLoader(mnist_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True)\n","\n","# train_set = torch.utils.data.dataset.TensorDataset(input_data, torch.zeros(input_data.size()[0]))\n","# train_loader = torch.utils.data.DataLoader(input_data, batch_size = batch_size, shuffle=True)\n","loss_function = nn.MSELoss()\n","loss_list = []\n","\n","for i in range(dbn.n_layers):\n","  print(dbn.n_layers)\n","  print(\"Training the %ith layer\"%int(i+1))\n","  optimizer = optim.SGD(dbn.rbm_layers[i].parameters(), lr = lr, weight_decay = weight_decay)\n","  if initialize_v:\n","    v = Variable(input_data)\n","    for ith in range(i):\n","        p_v, v = dbn.rbm_layers[ith].v_to_h(v)\n","    dbn.rbm_layers[i].v_bias.data.zero_()\n","    dbn.rbm_layers[i].v_bias.data.add_(torch.log(v.mean(0)/(1-v.mean(0))).data)\n","  for _ in range(epoch):\n","    total_loss = 0\n","    print(_)\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","#       data.view(data.size(0), -1)\n","      if batch_idx % 100 == 0:\n","        print(\"_\",batch_idx,end = \"\")\n","      data = data.view((data.shape[0] , -1)).type(torch.FloatTensor)\n","#       print(data.shape)\n","      v, v_ = dbn(v_input = data, ith_layer = i, CD_k = CD_k)\n","      _, redata =  dbn.reconstruct(data)\n","#       loss = dbn.rbm_layers[i].free_energy(v.detach()) - dbn.rbm_layers[i].free_energy(v_.detach()) + L1_penalty * torch.sum(torch.abs(dbn.rbm_layers[i].W))\n","      loss = loss_function(redata, data)\n","      total_loss += loss.item()\n","#       print(total_loss)\n","      loss.backward()\n","      optimizer.step()\n","      optimizer.zero_grad()\n","      \n","    print(\"\\nepoch: \", _, total_loss)\n","    if i == dbn.n_layers -1:\n","      loss_list.append(total_loss)\n","    \n","\n","  \n","  \n","#       print(redata[0])\n","#       org_imgs.append(data[0])\n","#       regen_imgs.append(redata[0])\n","#       print(type(redata))\n","#       if batch_idx % 1000 == 0:\n","#         print(\"______\",batch_idx,\"_\",loss_function(redata, data),'_',loss, end = \"\")\n","#       mse += loss\n","#       count += 1\n","#     mse /= float(count)\n","#     print(\"\\nepoch %i: \"%i,\" MSE: \", mse)\n","#     torg = torch.zeros(len(org_imgs),org_imgs[0].shape[0],org_imgs[0].shape[1]).cuda()\n","#     rorg = torch.zeros(len(org_imgs),org_imgs[0].shape[0],org_imgs[0].shape[1]).cuda()\n","#     org_imgs = torch.Tensor(org_imgs)\n","#     regen_imgs = torch.Tensor(regen_imgs)\n","    \n","#     i = 0\n","#     for img in org_imgs:\n","#       torg[i] = img\n","#       i += 1\n","#     i = 0\n","#     for img in regen_imgs:\n","#       rorg[i] = img\n","#       i += 1\n","#     print()\n","#     print(loss_function(regen_imgs, org_imgs))\n","    \n","#     org_imgs = []\n","#     regen_imgs = []\n","#     print(\"epoch %i: \"%i,\" Training loss: \",loss)\n","#     if not type(test_set) == type(None):\n","      \n","#       print(\"epoch %i: \"%i, loss_function(dbn.reconstruct(input_data), test_set))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["7\n","Training the 1th layer\n","0\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4969, 0.5003, 0.4972,  ..., 0.5010, 0.5006, 0.5019],\n","        [0.4967, 0.5000, 0.4994,  ..., 0.4990, 0.5002, 0.5007],\n","        [0.4963, 0.4998, 0.5005,  ..., 0.4977, 0.4985, 0.5027],\n","        ...,\n","        [0.4965, 0.5003, 0.4983,  ..., 0.5013, 0.4999, 0.5019],\n","        [0.4978, 0.5007, 0.4981,  ..., 0.5014, 0.4984, 0.5028],\n","        [0.4980, 0.4975, 0.5007,  ..., 0.5014, 0.4982, 0.5018]],\n","       grad_fn=<SigmoidBackward>) 3609.6565956771374\n","1\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4951, 0.4984, 0.4979,  ..., 0.4999, 0.5007, 0.5053],\n","        [0.4978, 0.4983, 0.5004,  ..., 0.5001, 0.4977, 0.5013],\n","        [0.4978, 0.4992, 0.4988,  ..., 0.5000, 0.4997, 0.4995],\n","        ...,\n","        [0.4961, 0.4992, 0.5001,  ..., 0.5022, 0.4978, 0.5033],\n","        [0.4960, 0.4991, 0.4991,  ..., 0.4992, 0.4979, 0.5004],\n","        [0.4979, 0.4993, 0.5002,  ..., 0.5007, 0.4974, 0.5028]],\n","       grad_fn=<SigmoidBackward>) 3610.2250624001026\n","2\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4960, 0.5000, 0.5003,  ..., 0.4995, 0.4974, 0.5006],\n","        [0.4963, 0.4982, 0.4985,  ..., 0.4991, 0.4991, 0.5015],\n","        [0.4977, 0.5008, 0.5010,  ..., 0.4993, 0.5006, 0.5009],\n","        ...,\n","        [0.4939, 0.5010, 0.5010,  ..., 0.5003, 0.4969, 0.5003],\n","        [0.4963, 0.4995, 0.5018,  ..., 0.5018, 0.4993, 0.5017],\n","        [0.4981, 0.5008, 0.4985,  ..., 0.5005, 0.4966, 0.5029]],\n","       grad_fn=<SigmoidBackward>) 3610.4618666768074\n","3\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4972, 0.4996, 0.4989,  ..., 0.5024, 0.4995, 0.5001],\n","        [0.4983, 0.5013, 0.5000,  ..., 0.5013, 0.5001, 0.5023],\n","        [0.4967, 0.5016, 0.4980,  ..., 0.5009, 0.4987, 0.5042],\n","        ...,\n","        [0.4970, 0.5002, 0.5009,  ..., 0.5022, 0.4977, 0.5006],\n","        [0.4961, 0.4997, 0.4980,  ..., 0.5017, 0.5008, 0.5023],\n","        [0.4954, 0.5010, 0.4977,  ..., 0.4981, 0.4977, 0.5006]],\n","       grad_fn=<SigmoidBackward>) 3609.0551413595676\n","4\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4979, 0.5001, 0.5001,  ..., 0.4986, 0.4999, 0.5004],\n","        [0.4933, 0.4994, 0.5000,  ..., 0.5001, 0.4983, 0.4997],\n","        [0.4985, 0.5004, 0.4995,  ..., 0.4994, 0.5008, 0.5014],\n","        ...,\n","        [0.4962, 0.4989, 0.5026,  ..., 0.5011, 0.5004, 0.5006],\n","        [0.4978, 0.5016, 0.4997,  ..., 0.5011, 0.4993, 0.5052],\n","        [0.4957, 0.4969, 0.4998,  ..., 0.4982, 0.4991, 0.4995]],\n","       grad_fn=<SigmoidBackward>) 3610.077860504389\n","5\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4959, 0.4982, 0.5000,  ..., 0.4998, 0.5000, 0.5039],\n","        [0.4957, 0.5003, 0.5008,  ..., 0.4990, 0.4996, 0.5022],\n","        [0.4961, 0.4994, 0.4972,  ..., 0.4994, 0.5004, 0.5012],\n","        ...,\n","        [0.4961, 0.5014, 0.4984,  ..., 0.5004, 0.5007, 0.5043],\n","        [0.4976, 0.4994, 0.5003,  ..., 0.5012, 0.5007, 0.5004],\n","        [0.4964, 0.4989, 0.5000,  ..., 0.5004, 0.4999, 0.5040]],\n","       grad_fn=<SigmoidBackward>) 3609.238051176071\n","6\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4964, 0.5023, 0.4968,  ..., 0.4995, 0.5006, 0.5006],\n","        [0.4984, 0.5008, 0.5019,  ..., 0.4990, 0.4971, 0.5020],\n","        [0.4968, 0.4987, 0.5012,  ..., 0.4990, 0.4992, 0.5013],\n","        ...,\n","        [0.4974, 0.4973, 0.4981,  ..., 0.5016, 0.5002, 0.5000],\n","        [0.4956, 0.4984, 0.5007,  ..., 0.5010, 0.4976, 0.5024],\n","        [0.4982, 0.4997, 0.4992,  ..., 0.4992, 0.5016, 0.5014]],\n","       grad_fn=<SigmoidBackward>) 3609.5118584930897\n","7\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4966, 0.4991, 0.5015,  ..., 0.5032, 0.4992, 0.5010],\n","        [0.4966, 0.5001, 0.5001,  ..., 0.5000, 0.4991, 0.4987],\n","        [0.4977, 0.4994, 0.4967,  ..., 0.5013, 0.4968, 0.5042],\n","        ...,\n","        [0.4958, 0.4992, 0.4989,  ..., 0.5005, 0.4998, 0.5002],\n","        [0.4952, 0.5002, 0.4995,  ..., 0.4991, 0.4984, 0.5025],\n","        [0.4963, 0.4978, 0.4995,  ..., 0.5002, 0.5000, 0.5004]],\n","       grad_fn=<SigmoidBackward>) 3610.3179193735123\n","8\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4964, 0.4981, 0.4996,  ..., 0.4999, 0.4976, 0.5016],\n","        [0.4970, 0.4990, 0.4998,  ..., 0.4993, 0.5023, 0.5031],\n","        [0.4946, 0.4995, 0.4979,  ..., 0.4992, 0.5001, 0.5011],\n","        ...,\n","        [0.4953, 0.4996, 0.5009,  ..., 0.5010, 0.4981, 0.5021],\n","        [0.4972, 0.5007, 0.4988,  ..., 0.5001, 0.4969, 0.5017],\n","        [0.4957, 0.5025, 0.5007,  ..., 0.5011, 0.4995, 0.5022]],\n","       grad_fn=<SigmoidBackward>) 3610.6967249810696\n","9\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4943, 0.5012, 0.4998,  ..., 0.5002, 0.4984, 0.5017],\n","        [0.4956, 0.5007, 0.5012,  ..., 0.5016, 0.5004, 0.5009],\n","        [0.4960, 0.5016, 0.5007,  ..., 0.5012, 0.4984, 0.5033],\n","        ...,\n","        [0.4971, 0.5012, 0.4982,  ..., 0.5024, 0.4984, 0.5042],\n","        [0.4954, 0.4984, 0.4981,  ..., 0.5012, 0.4988, 0.5004],\n","        [0.4970, 0.4999, 0.4969,  ..., 0.4988, 0.4984, 0.5021]],\n","       grad_fn=<SigmoidBackward>) 3609.9513590037823\n","7\n","Training the 2th layer\n","0\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4974, 0.4997, 0.4990,  ..., 0.5015, 0.4978, 0.5020],\n","        [0.4968, 0.4962, 0.5004,  ..., 0.5010, 0.4990, 0.5022],\n","        [0.4953, 0.4996, 0.4972,  ..., 0.5034, 0.4978, 0.5018],\n","        ...,\n","        [0.4962, 0.5017, 0.4984,  ..., 0.4992, 0.4983, 0.5028],\n","        [0.4973, 0.5006, 0.4998,  ..., 0.5010, 0.4993, 0.5014],\n","        [0.4969, 0.5014, 0.4988,  ..., 0.5020, 0.4985, 0.5025]],\n","       grad_fn=<SigmoidBackward>) 3609.409124046564\n","1\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4959, 0.4992, 0.4995,  ..., 0.5010, 0.4991, 0.5032],\n","        [0.4965, 0.5013, 0.5010,  ..., 0.5005, 0.4994, 0.5027],\n","        [0.4979, 0.4996, 0.4987,  ..., 0.5009, 0.4990, 0.5037],\n","        ...,\n","        [0.4970, 0.4990, 0.5015,  ..., 0.5010, 0.5004, 0.5018],\n","        [0.4983, 0.4981, 0.4985,  ..., 0.4998, 0.5003, 0.5039],\n","        [0.4981, 0.5010, 0.4977,  ..., 0.5020, 0.4987, 0.5020]],\n","       grad_fn=<SigmoidBackward>) 3609.029811233282\n","2\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4965, 0.5006, 0.4997,  ..., 0.5011, 0.4994, 0.5019],\n","        [0.4970, 0.4992, 0.4984,  ..., 0.5018, 0.4989, 0.5030],\n","        [0.4936, 0.5002, 0.4971,  ..., 0.5006, 0.4998, 0.5013],\n","        ...,\n","        [0.4954, 0.5009, 0.4986,  ..., 0.5001, 0.5012, 0.5032],\n","        [0.4954, 0.4981, 0.4998,  ..., 0.4996, 0.4980, 0.5000],\n","        [0.4975, 0.4990, 0.4989,  ..., 0.5007, 0.4985, 0.5035]],\n","       grad_fn=<SigmoidBackward>) 3610.2158761024475\n","3\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4969, 0.5004, 0.4989,  ..., 0.4983, 0.4986, 0.5010],\n","        [0.4970, 0.5020, 0.4995,  ..., 0.4999, 0.4992, 0.5031],\n","        [0.4962, 0.5000, 0.4989,  ..., 0.5025, 0.4996, 0.5001],\n","        ...,\n","        [0.4979, 0.4985, 0.4996,  ..., 0.5008, 0.4984, 0.5037],\n","        [0.4990, 0.4997, 0.4982,  ..., 0.4999, 0.4996, 0.5008],\n","        [0.4994, 0.5016, 0.4986,  ..., 0.4993, 0.5011, 0.5011]],\n","       grad_fn=<SigmoidBackward>) 3609.6744625270367\n","4\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4964, 0.5003, 0.5013,  ..., 0.4987, 0.4989, 0.5014],\n","        [0.4985, 0.5017, 0.4990,  ..., 0.4998, 0.5024, 0.5034],\n","        [0.4964, 0.5014, 0.4966,  ..., 0.5020, 0.5000, 0.5026],\n","        ...,\n","        [0.4965, 0.5004, 0.4975,  ..., 0.4994, 0.5002, 0.5019],\n","        [0.4952, 0.4995, 0.4989,  ..., 0.4990, 0.5003, 0.5022],\n","        [0.4943, 0.4999, 0.4987,  ..., 0.5007, 0.5001, 0.4993]],\n","       grad_fn=<SigmoidBackward>) 3610.4763053655624\n","5\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4969, 0.4997, 0.4990,  ..., 0.5028, 0.4971, 0.5012],\n","        [0.4966, 0.4975, 0.5003,  ..., 0.5025, 0.4989, 0.4992],\n","        [0.4971, 0.4987, 0.4994,  ..., 0.5010, 0.4989, 0.4998],\n","        ...,\n","        [0.4962, 0.5001, 0.4991,  ..., 0.4985, 0.5018, 0.5019],\n","        [0.4971, 0.4994, 0.4994,  ..., 0.4994, 0.4992, 0.5037],\n","        [0.4970, 0.5024, 0.5002,  ..., 0.4996, 0.4993, 0.5001]],\n","       grad_fn=<SigmoidBackward>) 3609.2143082618713\n","6\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4959, 0.5006, 0.4998,  ..., 0.5008, 0.4970, 0.5006],\n","        [0.4961, 0.4995, 0.4971,  ..., 0.5028, 0.4994, 0.5003],\n","        [0.4944, 0.4991, 0.4997,  ..., 0.5003, 0.4994, 0.5027],\n","        ...,\n","        [0.4981, 0.4998, 0.4995,  ..., 0.5023, 0.4981, 0.5012],\n","        [0.4959, 0.4999, 0.5005,  ..., 0.5000, 0.4992, 0.5012],\n","        [0.4982, 0.5013, 0.4999,  ..., 0.5001, 0.4974, 0.5026]],\n","       grad_fn=<SigmoidBackward>) 3610.01561370492\n","7\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4948, 0.5019, 0.4970,  ..., 0.5015, 0.4978, 0.5037],\n","        [0.4970, 0.4998, 0.5009,  ..., 0.5001, 0.4993, 0.5027],\n","        [0.4964, 0.5020, 0.5012,  ..., 0.5009, 0.4989, 0.5012],\n","        ...,\n","        [0.4964, 0.5005, 0.4980,  ..., 0.4995, 0.4988, 0.5016],\n","        [0.4951, 0.4998, 0.4996,  ..., 0.4994, 0.5006, 0.5008],\n","        [0.4968, 0.4997, 0.5001,  ..., 0.5006, 0.4994, 0.5024]],\n","       grad_fn=<SigmoidBackward>) 3609.6136088073254\n","8\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4949, 0.5011, 0.4989,  ..., 0.4980, 0.4968, 0.5034],\n","        [0.4964, 0.4991, 0.4999,  ..., 0.4992, 0.4997, 0.5017],\n","        [0.4950, 0.5002, 0.5003,  ..., 0.5032, 0.4993, 0.4990],\n","        ...,\n","        [0.4961, 0.5013, 0.5004,  ..., 0.4985, 0.4990, 0.5011],\n","        [0.4969, 0.4992, 0.4986,  ..., 0.5001, 0.4994, 0.5021],\n","        [0.4948, 0.4979, 0.4996,  ..., 0.5008, 0.4984, 0.4992]],\n","       grad_fn=<SigmoidBackward>) 3610.6562838852406\n","9\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4960, 0.5038, 0.4980,  ..., 0.5014, 0.4982, 0.5008],\n","        [0.4991, 0.4994, 0.5003,  ..., 0.5008, 0.4990, 0.5016],\n","        [0.4958, 0.4993, 0.5001,  ..., 0.5009, 0.4981, 0.5000],\n","        ...,\n","        [0.4940, 0.5005, 0.4981,  ..., 0.5003, 0.5003, 0.4996],\n","        [0.4964, 0.4984, 0.5016,  ..., 0.4989, 0.4995, 0.5034],\n","        [0.4977, 0.4996, 0.5001,  ..., 0.5014, 0.4988, 0.4998]],\n","       grad_fn=<SigmoidBackward>) 3610.121380060911\n","7\n","Training the 3th layer\n","0\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4950, 0.4989, 0.4978,  ..., 0.5007, 0.4980, 0.5006],\n","        [0.4970, 0.4982, 0.5004,  ..., 0.5002, 0.4998, 0.5013],\n","        [0.4959, 0.4987, 0.4994,  ..., 0.5011, 0.4988, 0.5002],\n","        ...,\n","        [0.4958, 0.5024, 0.4996,  ..., 0.5021, 0.4988, 0.5009],\n","        [0.4962, 0.5003, 0.4995,  ..., 0.5023, 0.5011, 0.5043],\n","        [0.4945, 0.5014, 0.4981,  ..., 0.5010, 0.4999, 0.5001]],\n","       grad_fn=<SigmoidBackward>) 3609.2682861089706\n","1\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300_ 1400_ 1500_ 1600_ 1700_ 1800_ 1900_ 2000_ 2100_ 2200_ 2300_ 2400_ 2500_ 2600_ 2700_ 2800_ 2900_ 3000_ 3100_ 3200_ 3300_ 3400_ 3500_ 3600_ 3700_ 3800_ 3900_ 4000_ 4100_ 4200_ 4300_ 4400_ 4500_ 4600_ 4700_ 4800_ 4900_ 5000_ 5100_ 5200_ 5300_ 5400_ 5500_ 5600_ 5700_ 5800_ 5900_ 6000_ 6100_ 6200_ 6300_ 6400_ 6500_ 6600_ 6700_ 6800_ 6900_ 7000_ 7100_ 7200_ 7300_ 7400\n","epoch:  tensor([[0.4943, 0.5015, 0.4992,  ..., 0.5005, 0.4988, 0.5009],\n","        [0.4957, 0.5003, 0.4986,  ..., 0.5009, 0.4985, 0.5014],\n","        [0.4993, 0.5026, 0.4985,  ..., 0.5011, 0.4973, 0.5013],\n","        ...,\n","        [0.4970, 0.5003, 0.4991,  ..., 0.4987, 0.5011, 0.4989],\n","        [0.4955, 0.4976, 0.5008,  ..., 0.5008, 0.5008, 0.4995],\n","        [0.4956, 0.4988, 0.4983,  ..., 0.4993, 0.5011, 0.5049]],\n","       grad_fn=<SigmoidBackward>) 3609.859666854143\n","2\n","_ 0_ 100_ 200_ 300_ 400_ 500_ 600_ 700_ 800_ 900_ 1000_ 1100_ 1200_ 1300"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-79-cf2aa09b8ec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m#       print(total_loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"rLB0PNuS9pcd","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}